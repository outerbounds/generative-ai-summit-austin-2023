{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can change the response from the models without changing the models!\n",
    "\n",
    "In this lesson we will explore ways to make LLM responses more relevant to end users through prompt engineering. To set the stage, consider the analog in traditional search engines, such as how Google provides us a [variety](https://support.google.com/websearch/answer/35890?sjid=5982066035738547434-NA) [of](https://support.google.com/websearch/answer/142143?sjid=5982066035738547434-NA) [ways](https://blog.google/products/search/how-were-improving-search-results-when-you-use-quotes/) to make queries more precise. LLMs are similar in that there are specific ways to write prompts that influence LLM APIs and the downstream Chat bots.\n",
    "\n",
    "You will learn\n",
    "- how to modify your ChatGPT interface to get it to do more of what you want,\n",
    "- how to implement the same approach programmatically with a trending framework called [Langchain](https://python.langchain.com/docs/get_started/introduction), and\n",
    "- a basic introduction to different fields of research around prompting - centered around \"chain-of-thought reasoning\".\n",
    "\n",
    "> \"In the vast majority of cases, we believe well-crafted prompts will get you the results you want\" - [Anthropic documentation](https://www.anthropic.com/product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Messages\n",
    "\n",
    "System prompts are used to steer the way that ChatGPT responds to questions. Here is a [repo of examples](https://github.com/mustvlad/ChatGPT-System-Prompts).\n",
    " Not all of the LLM models will respond to system prompts like ChatGPT/GPT-4, where this is an explicit feature of the product. Later we will see how to do these things programmatically. If you use ChatGPT regularly, you should know about this feature of the GPT family and LLMs generally!\n",
    "\n",
    "### Get started with ChatGPT\n",
    "Open [ChatGPT](https://chat.openai.com/)\n",
    "\n",
    "In the \"How would you like ChatGPT to respond?\" box:\n",
    "```\n",
    "You are a garden gnome that writes whimsical and highly informative poems about boring topics in computer programming and business, connecting these topics to the natural beauty of mother earth.\n",
    "\n",
    "Make rhymes.\n",
    "Make responses fun and lighthearted.\n",
    "Make responses precise and accurate.\n",
    "```\n",
    "\n",
    "### The recipe\n",
    "Then ask a boring question about computer programming or business and see what happens!\n",
    "\n",
    "This is a silly example to help you get started, and [here](https://betterprogramming.pub/i-know-you-have-been-trained-up-to-2021-chatgpt-system-messages-explained-146a5513e753) is a more serious guide containing insights such as:\n",
    "- Clearly define the role you want ChatGPT to play\n",
    "- Clearly define the tone and format of the output\n",
    "- Be explicit and add context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templating\n",
    "Prompt templates are in the realm of recent tools like Langchain, designed to help us interface with LLMs.\n",
    "Essentially, prompt templates extend the idea of the system message into something like superpowered string formatting or jinja templating.\n",
    "\n",
    "> \"Prompt templates are pre-defined recipes for generating prompts for language models. A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\" - [Langchain docs](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qqq langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from langchain docs: https://python.langchain.com/docs/get_started/quickstart\n",
    "\n",
    "# langchain dependencies\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "# See models - https://python.langchain.com/docs/integrations/chat/\n",
    "# Note: not all the models interact with system messages in the same way!\n",
    "# You have to learn about how the specific model you are interested in behaves.\n",
    "\n",
    "# create the template/format\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language} with many years of experience teaching beginner language students.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich liebe Programmieren.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(inp=\"English\", out=\"German\", text=\"I love programming?\"):\n",
    "    response = chat(chat_prompt.format_messages(input_language=inp, output_language=out, text=text))\n",
    "    return response.content\n",
    "\n",
    "translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmatic approaches to prompting\n",
    "\n",
    "There are many variants on prompt engineering research over the last year. \n",
    "\n",
    "| Title | Medium | Organizations | Date | Code | Method |\n",
    "| :--- | :---: | :---: | :---: | :---: | :---: |\n",
    "| [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/pdf/2110.08387.pdf) | Paper | U Washington, Allen AI | September 2022 | https://github.com/liujch1998/GKP |\n",
    "| [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) | Paper | Google Brain | January 2023 | | Chain of Thought |\n",
    "| [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435) | Paper | CMU | January 2023 |  | \n",
    "| [Chain of Thought Paradigms in LLMs](https://matt-rickard.com/chain-of-thought-in-llms) | Blog |  | March 2023 |  | Chain of Thought |\n",
    "| [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629.pdf)| Paper| Google Brain, Princeton| March 2023| | ReAct |\n",
    "| [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171) | Paper| Google | March 2023| | |\n",
    "| [ART: Automatic multi-step reasoning and tool-use for large language models](https://arxiv.org/pdf/2303.09014.pdf) | Paper | U Washington, Microsoft, UC Irvine, Allen AI | March 2023 | | Automatic Reasoning and Tool Use |\n",
    "| [Least-to-most prompting enables complex reasoning in large language models](https://arxiv.org/pdf/2205.10625.pdf) | Paper | Google Brain | April 2023 |  | \n",
    "| [Measuring and Narrowing the Compositionality Gap in Language Models](https://arxiv.org/pdf/2210.03350.pdf) | Paper | U Washington, MosaicML, Meta AI, Allen AI | May 2023 | https://github.com/ofirpress/self-ask | Self-ask Prompting | \n",
    "| [Large Language Models are Zero-Shot Rankers for Recommender Systems](https://arxiv.org/pdf/2305.08845.pdf) | Paper | Tencent | May 2023 |  | \n",
    "| [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601) | Paper | Deepmind, Princeton | May 2023 | https://github.com/princeton-nlp/tree-of-thought-llm | Chain of Thought |\n",
    "| [Unraveling the Power of Chain-of-Thought Prompting in Large Language Models](https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html) | Blog | KDNuggets | July 2023 |  | Chain of Thought | \n",
    "| [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/pdf/2309.04269.pdf) | Paper | Salesforce, Columbia, MIT | September 2023 |  | Chain of Density | \n",
    "| [Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution](https://arxiv.org/abs/2309.16797?fbclid=IwAR1o-VI0DSwNOawBAQAcv0adoDakSWrgwPuLxWqJhLdCbouuZBA0Gm7Sy8I) | Paper | Deepmind | October 2023 |  | Evolutionary Algorithm |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, we introduced methods for changing prompts to modify the behavior of LLM APIs. \n",
    "You learned how to set up system messages for ChatGPT, use prompt templates with Langchain, and about some research directions in programmatic prompt engineering.\n",
    "\n",
    "In the next section, we will take a step back from prompt structures and think about what information is going into the prompt.\n",
    "More specifically you will learn about how to condition LLM continuations on your data with retrieval-augmented generation (RAG) pipelines, and the latest infrastructure people are using to implement these patterns at scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
