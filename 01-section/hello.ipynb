{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, pipeline\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model & Convert to Optimum\n",
    "\n",
    "In this section we will see how to load a pre-trained model from the HuggingFace Hub. \n",
    "You can shop for models [here](https://huggingface.co/models).\n",
    "\n",
    "Then it will be coverted using the [`BetterTransformer`](https://huggingface.co/docs/optimum/bettertransformer/overview) from the [optimum project](https://huggingface.co/docs/optimum/index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\" \n",
    "model = AutoModel.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# convert to BetterTransformer format to speed up inference\n",
    "bt_model = BetterTransformer.transform(model, keep_original_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted_model:  RobertaModel(\n",
      "  (embeddings): RobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): RobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayerBetterTransformer(\n",
      "        (act_fn_callable): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): RobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"converted_model: \", bt_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Pipeline API\n",
    "\n",
    "In the previous section we saw how to load a model, in this section we see the easiest way to use HuggingFace models for inference.\n",
    "\n",
    "Specifically, we will show the following APIs of the [HuggingFace Pipeline API](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines) and its cousin from the [optimum project](https://huggingface.co/docs/optimum/index), a collaboration between HuggingFace and PyTorch which improves inference latency with no performance hit:\n",
    "* [Text Classsification](#text-classification)\n",
    "* [Text Generation](#text-generation)\n",
    "* [Text Mask Fill - Optimum](#optimum-for-faster-latency)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More text classification models: https://huggingface.co/models?pipeline_tag=text-classification&sort=trending\n",
    "model_name = \"SamLowe/roberta-base-go_emotions\" \n",
    "classifier_pipe = pipeline(\"text-classification\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I am feeling inspired today.\",\n",
    "    \"This talk is informative, but a bit high-level, where I can find more details?\",\n",
    "    \"I wonder about all the hype around Generative AI, is smoke and mirrors?\",\n",
    "    \"Building production machine learning systems is challenging.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'excitement', 'score': 0.24082745611667633},\n",
       " {'label': 'admiration', 'score': 0.5622110366821289},\n",
       " {'label': 'curiosity', 'score': 0.5444050431251526},\n",
       " {'label': 'neutral', 'score': 0.46457335352897644}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_pipe(sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloom-560m\" # https://huggingface.co/bigscience/bloom-560m\n",
    "generator = pipeline(\"text-generation\", model=model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The Generative AI World Summit is a\"\n",
    "response = generator(prompt, do_sample=False, max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Prompt**: The Generative AI World Summit is a\n",
       "\n",
       "**bigscience/bloom-560m's continuation**: The Generative AI World Summit is a global conference that brings together the best in AI, Machine Learning, and Artificial Intelligence to discuss the future of AI and how it...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(f\"\"\"\n",
    "**Prompt**: {prompt}\n",
    "\n",
    "**{model_name}'s continuation**: {response[0]['generated_text']}...\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimum for Faster Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "/home/workspace/mambaforge/envs/sandbox-tutorial/lib/python3.10/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1695391884920/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)\n"
     ]
    }
   ],
   "source": [
    "from optimum.pipelines import pipeline\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "prompt = \"I am attending the Generative AI Summit and I am a practicing [MASK].\"\n",
    "\n",
    "unmasked_optimum_pipeline = pipeline(task=\"fill-mask\", model=model_name, accelerator=\"bettertransformer\")\n",
    "response = unmasked_optimum_pipeline(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Token ID</th>\n",
       "      <th>Token mask fill</th>\n",
       "      <th>Full generated text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.110810</td>\n",
       "      <td>15034</td>\n",
       "      <td>psychologist</td>\n",
       "      <td>i am attending the generative ai summit and i am a practicing psychologist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078805</td>\n",
       "      <td>13235</td>\n",
       "      <td>mathematician</td>\n",
       "      <td>i am attending the generative ai summit and i am a practicing mathematician.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.051947</td>\n",
       "      <td>7992</td>\n",
       "      <td>buddhist</td>\n",
       "      <td>i am attending the generative ai summit and i am a practicing buddhist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047386</td>\n",
       "      <td>7155</td>\n",
       "      <td>scientist</td>\n",
       "      <td>i am attending the generative ai summit and i am a practicing scientist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.045670</td>\n",
       "      <td>21477</td>\n",
       "      <td>biologist</td>\n",
       "      <td>i am attending the generative ai summit and i am a practicing biologist.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Score  Token ID Token mask fill  \\\n",
       "0  0.110810  15034     psychologist     \n",
       "1  0.078805  13235     mathematician    \n",
       "2  0.051947  7992      buddhist         \n",
       "3  0.047386  7155      scientist        \n",
       "4  0.045670  21477     biologist        \n",
       "\n",
       "                                                            Full generated text  \n",
       "0  i am attending the generative ai summit and i am a practicing psychologist.   \n",
       "1  i am attending the generative ai summit and i am a practicing mathematician.  \n",
       "2  i am attending the generative ai summit and i am a practicing buddhist.       \n",
       "3  i am attending the generative ai summit and i am a practicing scientist.      \n",
       "4  i am attending the generative ai summit and i am a practicing biologist.      "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "col_mapping = {\"score\": \"Score\", \"token_str\": \"Token mask fill\", \"token\": \"Token ID\", \"sequence\": \"Full generated text\"}\n",
    "pd.DataFrame(response).rename(columns=col_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
