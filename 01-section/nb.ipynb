{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To execute this notebook, choose the `llm-rag` kernel in the dropdown above. You may need to hit the \"Select another kernel\" button, and refresh the kernels list.*\n",
    "\n",
    "# Getting off the Ground with LLMs\n",
    "\n",
    "In this section, we will walk through how to access cutting-edge LLMs from your Python codes.\n",
    "We will walk through the basics of commercial APIs, open-source APIs, and a bit about their relative capabilities.\n",
    "\n",
    "## Commercial APIs\n",
    "\n",
    "If you don't have a plan, but want to build a language model-driven solution, commercial APIs are a good way to start.\n",
    "\n",
    "This section shows you how to get started with the leading commercial APIs as of October 2023:\n",
    "- [OpenAI](https://platform.openai.com/docs/api-reference)\n",
    "- [Cohere](https://docs.cohere.com/reference/about)\n",
    "- [Jurassic-2](https://docs.ai21.com/reference/python-sdk) from [A21 Labs](https://www.ai21.com/)\n",
    "- [Claude](https://github.com/anthropics/anthropic-sdk-python) from Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notebook display\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "### Data processing\n",
    "import pandas as pd\n",
    "\n",
    "### Commerical LLM providers\n",
    "import openai\n",
    "import cohere\n",
    "import ai21\n",
    "\n",
    "### Open-source LLMs\n",
    "from transformers import AutoModel, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API Keys\n",
    " \n",
    "Go to these links to find your tokens (after signing up):\n",
    "- [OpenAI](https://platform.openai.com/account/api-keys)\n",
    "- [Cohere](https://dashboard.cohere.com/api-keys)\n",
    "- [A21 Labs](https://studio.ai21.com/account/api-key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = ...\n",
    "cohere_key = ...\n",
    "ai21_key = ...\n",
    "# Note: Did not include Claude here, as SDK/API access is gated: https://docs.anthropic.com/claude/docs/getting-access-to-claude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The common prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"How is generative AI affecting the infrastrucutre machine learning developers need access to?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_key)\n",
    "gpt35_completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[{\"role\": \"user\", \"content\": PROMPT}]\n",
    ")\n",
    "gpt35_text_response = gpt35_completion.to_dict()['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_text_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello Cohere API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(cohere_key)\n",
    "cohere_cmd_completion = co.generate(prompt=PROMPT, model=\"command\")\n",
    "cohere_cmd_response = cohere_cmd_completion.generations[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cohere_cmd_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello A21 Labs API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai21 import AI21Client\n",
    "client = AI21Client(api_key=ai21_key)\n",
    "response_mid = client.completion.create(\n",
    "  model=\"j2-mid\",\n",
    "  prompt=PROMPT,\n",
    "  num_results=1,\n",
    "  max_tokens=100,\n",
    "  temperature=0.4,\n",
    "  top_k_return=0,\n",
    "  top_p=1,\n",
    "  stop_sequences=[\"##\"]\n",
    ")\n",
    "jurassic2_response = response_mid.completions[0].data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(f\"\"\"\n",
    "\n",
    "**OpenAI GPT3.5**: {gpt35_text_response}\n",
    "\n",
    "{\"=\"*160}\n",
    "\n",
    "**Cohere Command**: {cohere_cmd_response}\n",
    "\n",
    "{\"=\"*160}\n",
    "\n",
    "**AI21 Jurassic2**: {jurassic2_response}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint support across APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **rough** picture of what endpoints these APIs have available as of October 20, 2023, without much more effort than what you just saw.\n",
    "\n",
    "> Note: You can make each of these models do almost anything, making this all muddy. <br/>The point of this table is to highlight which of these APIs have documented endpoints for certain tasks.\n",
    "\n",
    "<center>\n",
    "\n",
    "| Endpoint / API | OpenAI | Cohere | Claude | A21 |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| Prompt-to-response | ✅ | ✅ | ✅ | ✅ |\n",
    "| Chat-to-response | ✅ | ✅ | ✅ | ✅ |\n",
    "| Text embeddings | ✅ | ✅ | ❌ | ✅ |\n",
    "| Fine-tuning | ✅ | ✅ | ❌ | ✅ |\n",
    "| Language detection | ❌ | ✅ | ❌ | ❌ |\n",
    "| Raw document processing | ✅ | ✅ | ❌ | ✅ | \n",
    "| Rerank / document relevance | ❌ | ✅ | ❌ | ✅ |\n",
    "| Text/image to image | ✅ | ❌ | ❌ | ❌ |\n",
    "| Audio-to-text | ✅ | ❌ | ❌ | ❌ |\n",
    "| Moderations / toxicitiy | ✅ | ✅ | ❌ | ❌ | \n",
    "\n",
    "</center>\n",
    "\n",
    "Some opinions related to this table:\n",
    "- If you want to pay for the **best chat model** --> \n",
    "    - OpenAI's GPT4 API is the current gold standard.\n",
    "- If you want **multimodal** --> \n",
    "    - OpenAI APIs are great - [Images](https://platform.openai.com/docs/api-reference/images), [Audio](https://platform.openai.com/docs/api-reference/audio)\n",
    "    -  Stability AI has some nice products not listed here. \n",
    "    - Check out this recent survey paper: [The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)](https://arxiv.org/abs/2309.17421)\n",
    "- If you care a lot about **content moderation** --> \n",
    "    - [Cohere](https://docs.cohere.com/docs/content-moderation-with-classify) and [OpenAI](https://platform.openai.com/docs/api-reference/moderations) have the most API support\n",
    "- If you want fine-grained **multi-lingual models** --> \n",
    "    - Try Cohere's [Multilingual Embedding](https://docs.cohere.com/docs/multilingual-language-models) APIs\n",
    "- If you want a model that can **analyze grammar** --> \n",
    "    - Try A21's [Text Improvements](https://docs.ai21.com/reference/text-improvements-api-ref) and [Grammatical Error Corrections](https://docs.ai21.com/reference/gec-api-ref) APIs\n",
    "- The public Claude product is a personal favorite, however their API access and feature support is lacking behind others in this list\n",
    "\n",
    "Of course, you can also try meshing them together if you have the budget and engineering will!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Open-source Models\n",
    "\n",
    "## Discussion\n",
    "- Why would you want to use open-source LLMs?\n",
    "- Will they ever really be competitive? \n",
    "    - What drives the competition if OpenAI's models are 10x bigger and performance keeps scaling with model size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "In this section we will see how to load a pre-trained model from the HuggingFace Hub. \n",
    "You can shop for models [here](https://huggingface.co/models).\n",
    "\n",
    "After, you'll see how to use these models for text classification and text generation, similar to the core mechanism of how the commerical APIs you saw above are generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 paper: https://arxiv.org/pdf/2210.11416.pdf\n",
    "model_name = \"t5-small\"\n",
    "model = AutoModel.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to learn more about transformers like the BERT and GPT family and how they work? \n",
    "- Sebastian Raschka recently gave his description of the history of the transformer in this concise [post](https://www.linkedin.com/posts/sebastianraschka_llms-largelanguagemodels-ai-activity-7121484400701186048--47t?utm_source=share&utm_medium=member_desktop).\n",
    "- Check out the amazing [Bertviz](https://github.com/jessevig/bertviz) tool by [jessevig](https://github.com/jessevig/). you can see a pre-loaded demo [here](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing#scrollTo=twSVFOM9SopW)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Pipeline API\n",
    "\n",
    "In the previous section we saw how to load a model, in this section we see the easiest way to use HuggingFace models for inference like with the earlier examples using commercial APIs.\n",
    "\n",
    "You will see how the [HuggingFace Pipeline API](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines) perform tasks including:\n",
    "* [Text Classsification](#text-classification)\n",
    "* [Text Generation](#text-generation)\n",
    "* many more tasks [here](https://huggingface.co/tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "https://huggingface.co/tasks/text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloom-560m\" # https://huggingface.co/bigscience/bloom-560m\n",
    "generator = pipeline(\"text-generation\", model=model_name, device_map=\"auto\")\n",
    "\n",
    "prompt = \"To learn MLOps in 2024, start by\" \n",
    "response = generator(prompt, do_sample=False, max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(f\"\"\"\n",
    "**Prompt**: {prompt}\n",
    "\n",
    "**{model_name}'s continuation**: {response[0]['generated_text']}...\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "https://huggingface.co/tasks/text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More text classification models: https://huggingface.co/models?pipeline_tag=text-classification&sort=trending\n",
    "model_name = \"SamLowe/roberta-base-go_emotions\" \n",
    "\n",
    "# Create a text classification pipeline using HuggingFace transformers pipeline.\n",
    "classifier_pipe = pipeline(\"text-classification\", model=model_name)\n",
    "\n",
    "# Sample data we want to classify the sentiment of.\n",
    "sentences = [\n",
    "    \"I am feeling inspired today. What a time to be alive!\",\n",
    "    \"This talk is informative, but a bit high-level, where I can find more details?\",\n",
    "    \"I wonder about all the hype around Generative AI, is it smoke and mirrors?\",\n",
    "    \"Building production-grade machine learning systems is challenging.\"\n",
    "]\n",
    "\n",
    "# Run the pipeline!\n",
    "classifier_pipe(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this lesson, you've learned:\n",
    "- how to programmatically query the leading commercial generative AI APIs\n",
    "- which endpoints are supported by the leading generative AI APIs\n",
    "- how to get started with replicating the core modeling loops of generative AI using open-source\n",
    "\n",
    "In the next lessons we will discuss methods for increasing the relevance of LLM responses, starting with basic prompt engineering, retrieval-augmented generation (RAG), and changing the model itself through fine-tuning and serving it behind an API you can control."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
