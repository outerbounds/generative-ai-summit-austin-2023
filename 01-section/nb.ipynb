{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting off the Ground with Commercial APIs\n",
    "\n",
    "If you don't have a plan, but want to build a language model-driven solution, commercial APIs are a good way to start.\n",
    "\n",
    "This section shows you how to get started with the leading commercial APIs as of October 2023:\n",
    "- [OpenAI](https://platform.openai.com/docs/api-reference)\n",
    "- [Cohere](https://docs.cohere.com/reference/about)\n",
    "- [Jurassic-2](https://docs.ai21.com/reference/python-sdk) from [A21 Labs](https://www.ai21.com/)\n",
    "- [Claude](https://github.com/anthropics/anthropic-sdk-python) from Anthropic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qqq openai cohere ai21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notebook display\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "### Data processing\n",
    "import pandas as pd\n",
    "\n",
    "### Commerical LLM providers\n",
    "import openai\n",
    "import cohere\n",
    "\n",
    "### Open-source LLMs\n",
    "from transformers import AutoModel, pipeline\n",
    "from optimum.bettertransformer import BetterTransformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API Keys\n",
    " \n",
    "Go to these links to find your tokens (after signing up):\n",
    "- [OpenAI](https://platform.openai.com/account/api-keys)\n",
    "- [Cohere](https://dashboard.cohere.com/api-keys)\n",
    "- [A21 Labs](https://studio.ai21.com/account/api-key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = ...\n",
    "cohere_key = ...\n",
    "ai21_key = ...\n",
    "# Note: Did not include Claude here, as SDK/API access is gated: https://docs.anthropic.com/claude/docs/getting-access-to-claude"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The common prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"How is generative AI affecting the infrastrucutre machine learning developers need access to?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = openai_key\n",
    "gpt35_completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[{\"role\": \"user\", \"content\": PROMPT}]\n",
    ")\n",
    "gpt35_text_response = gpt35_completion.to_dict()['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello Cohere API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(cohere_key)\n",
    "cohere_cmd_completion = co.generate(prompt=PROMPT, model=\"command\")\n",
    "cohere_cmd_response = cohere_cmd_completion.data[0].text.strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello A21 Labs API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai21\n",
    "ai21.api_key = ai21_key\n",
    "jurassic2_completion = ai21.Completion.execute(\n",
    "    model=\"j2-mid\", \n",
    "    prompt=PROMPT,\n",
    "    maxTokens=250,\n",
    ")\n",
    "jurassic2_response = jurassic2_completion.completions[0]['data']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**OpenAI GPT3.5**: Generative AI is significantly impacting the infrastructure required by machine learning developers. Here are a few key ways:\n",
       "\n",
       "1. Increased computational demands: Generative AI models, such as deep generative models and variational autoencoders, are often computationally intensive. These models require access to high-performance GPUs or even specialized hardware like TPUs (Tensor Processing Units) to accelerate training or inference. As a result, developers need access to powerful hardware infrastructure to train and deploy generative AI models effectively.\n",
       "\n",
       "2. Large-scale training data: Many generative AI models require a massive amount of training data to learn effectively. For instance, generative adversarial networks (GANs) need substantial datasets to capture the underlying distribution accurately. Machine learning developers need access to large, high-quality datasets, often stored in distributed file systems or cloud storage, and efficient data processing capabilities to train generative models.\n",
       "\n",
       "3. Advanced model architectures: Generative AI has introduced complex architectures like GANs, transformers, and autoregressive models that require sophisticated infrastructure for their development and deployment. These models often involve intricate architectures with numerous layers and attention mechanisms. Developers need access to capable deep learning frameworks and libraries, such as TensorFlow or PyTorch, and compatible infrastructure that supports these advanced model architectures.\n",
       "\n",
       "4. Efficient hyperparameter tuning: Generative AI models often have numerous hyperparameters, such as learning rates, batch sizes, or regularization terms. Tuning these hyperparameters is critical to achieving optimal performance. Machine learning developers require access to infrastructure that enables efficient hyperparameter optimization techniques like grid search, random search, or Bayesian optimization. This may involve distributed computing resources to parallelize hyperparameter search and training processes.\n",
       "\n",
       "5. Real-time or interactive inference: Some generative AI applications, like style transfer or text-to-image synthesis, require real-time or interactive inference capabilities. Developers need infrastructure with low-latency processing and high throughput to serve these applications quickly. This may involve deploying models on cloud-based systems, edge devices, or specialized hardware like GPUs or FPGAs (Field-Programmable Gate Arrays) to ensure responsive and interactive generative AI experiences.\n",
       "\n",
       "In summary, generative AI has driven the need for powerful computational resources, large-scale datasets, advanced model architectures, efficient hyperparameter tuning, and real-time inference capabilities—shaping the infrastructure requirements for machine learning developers.\n",
       "\n",
       "================================================================================================================================================================\n",
       "\n",
       "**Cohere Command**: As generative AI emerges as a powerful technology for creating new content and ideas, it is having a significant impact on the infrastructure and resources that machine learning (ML) developers need to create and train AI models. Here are some key ways in which generative AI is affecting the ML infrastructure:\n",
       "\n",
       "Data storage and management: Generative AI models can produce a large volume of data, which requires developers to have sufficient data storage and management infrastructure. This can include cloud-based storage solutions, as well as data processing and analysis tools to handle the large volume of data generated.\n",
       "\n",
       "Computational power: Training generative AI models requires significant computational power, including access to high-performance GPUs and other specialized hardware. This has led to the development of specialized cloud-based services and infrastructure for training AI models, such as Amazon Web Services (AWS) SageMaker and Google Cloud AI Platform.\n",
       "\n",
       "Model development and training: The development and training of generative AI models requires developers to have access to advanced machine learning frameworks and tools, such as TensorFlow, PyTorch, and MXNet. These frameworks provide the necessary infrastructure for building and training complex AI models, as well as for deploying them into production.\n",
       "\n",
       "Data preprocessing and cleaning: The quality of the data used to train generative AI models is critical to their performance. This has led to the development of new tools and techniques for data preprocessing and cleaning, including data augmentation and data normalization.\n",
       "\n",
       "Security and privacy: As generative AI models become more powerful, they may also become more vulnerable to security and privacy threats. This has led to the development of new security and privacy measures, such as secure data storage and encryption, as well as new techniques for protecting the privacy of users.\n",
       "\n",
       "Overall, the rise of generative AI is driving the development of new infrastructure and resources for machine learning developers, including advanced data storage and management solutions, high-performance computing resources, and advanced machine learning frameworks and tools.\n",
       "\n",
       "================================================================================================================================================================\n",
       "\n",
       "**AI21 Jurassic2**: \n",
       "Generative AI is impacting the infrastructure that machine learning developers need access to in several ways:\n",
       "\n",
       "1. Faster Training: Generative models require large amounts of data to train, and the availability of powerful computing resources is crucial for training them efficiently. With advancements in hardware and the emergence of specialized hardware like GPUs and TPUs, it is now possible to train generative models on larger datasets in shorter timeframes.\n",
       "2. Enhanced Storage: Generative models generate new data, which can significantly increase the demand for storage. As generative models become more prevalent in various applications, the need for specialized storage systems that can handle large datasets efficiently continues to grow.\n",
       "3. Improved Networking: Training generative models can involve the transfer of large amounts of data between the model and the training infrastructure. Faster networking infrastructure that can efficiently handle large data transfers is necessary for facilitating the smooth training of generative models.\n",
       "4. Security and Privacy Concerns: Generative models involve the processing of sensitive personal data, so the infrastructure and systems supporting them need to incorporate robust security and privacy measures. This includes encrypting data at rest and in transit, implementing access controls, and conducting regular security audits.\n",
       "5. Cloud Adoption: Generative models are computationally intensive and often require specialized hardware, which can be challenging to implement on-premises. As a result, many organizations are moving to cloud-based infrastructure, which offers scalable and easily accessible computing resources. Cloud providers often provide preconfigured environments for deep learning that make it easier for developers to train generative models.\n",
       "6. Collaboration and Sharing: Generative models often benefit from the collective efforts of\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(f\"\"\"\n",
    "\n",
    "**OpenAI GPT3.5**: {gpt35_text_response}\n",
    "\n",
    "{\"=\"*160}\n",
    "\n",
    "**Cohere Command**: {cohere_cmd_response}\n",
    "\n",
    "{\"=\"*160}\n",
    "\n",
    "**AI21 Jurassic2**: {jurassic2_response}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint support across APIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **rough** picture of what endpoints these APIs have available as of October 20, 2023, without much more effort than what you just saw.\n",
    "\n",
    "> Note: You can make each of these models do almost anything, making this all muddy. <br/>The point of this table is to highlight which of these APIs have documented endpoints for certain tasks.\n",
    "\n",
    "<center>\n",
    "\n",
    "| Endpoint / API | OpenAI | Cohere | Claude | A21 |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| Prompt-to-response | ✅ | ✅ | ✅ | ✅ |\n",
    "| Chat-to-response | ✅ | ✅ | ✅ | ✅ |\n",
    "| Text embeddings | ✅ | ✅ | ❌ | ✅ |\n",
    "| Fine-tuning | ✅ | ✅ | ❌ | ✅ |\n",
    "| Language detection | ❌ | ✅ | ❌ | ❌ |\n",
    "| Raw document processing | ✅ | ✅ | ❌ | ✅ | \n",
    "| Rerank / document relevance | ❌ | ✅ | ❌ | ✅ |\n",
    "| Text/image to image | ✅ | ❌ | ❌ | ❌ |\n",
    "| Audio-to-text | ✅ | ❌ | ❌ | ❌ |\n",
    "| Moderations / toxicitiy | ✅ | ✅ | ❌ | ❌ | \n",
    "\n",
    "</center>\n",
    "\n",
    "Some opinions underlying this table:\n",
    "- If you want to pay for the best chat model --> OpenAI's GPT4 API is gold standard\n",
    "- If you want multimodal --> OpenAI APIs are great. Stability AI has some nice products not listed here\n",
    "- If you care a lot about content moderation --> Cohere and OpenAI have the most API support\n",
    "- If you want fine-grained multi-lingual models --> Try Cohere's [Multilingual Embedding](https://docs.cohere.com/docs/multilingual-language-models) APIs\n",
    "- If you want build a model that can analyze grammar carefully --> Try A21's [Text Improvements](https://docs.ai21.com/reference/text-improvements-api-ref) and [Grammatical Error Corrections](https://docs.ai21.com/reference/gec-api-ref) APIs\n",
    "- The public Claude product is a personal favorite, however their API access and feature support is lacking behind others in this list\n",
    "\n",
    "Of course, you can also try meshing them together if you have the budget and engineering will!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Open-source Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "In this section we will see how to load a pre-trained model from the HuggingFace Hub. \n",
    "You can shop for models [here](https://huggingface.co/models).\n",
    "\n",
    "After, you'll see how to use these models for text classification and text generation, similar to the core mechanism of how the commerical APIs you saw above are generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199485ecbaa9463eb5f55d3edc6b5731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7009658cbbf94f1988122bbbbfebd2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\" \n",
    "model = AutoModel.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to learn more about transformers like the BERT and GPT family and how they work? Check out the amazing [Bertviz](https://github.com/jessevig/bertviz) tool by [jessevig](https://github.com/jessevig/). you can see a pre-loaded demo [here](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing#scrollTo=twSVFOM9SopW)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Pipeline API\n",
    "\n",
    "In the previous section we saw how to load a model, in this section we see the easiest way to use HuggingFace models for inference like with the earlier examples using commercial APIs.\n",
    "\n",
    "You will see how the [HuggingFace Pipeline API](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines) perform tasks including:\n",
    "* [Text Classsification](#text-classification)\n",
    "* [Text Generation](#text-generation)\n",
    "* [Text Mask Fill - Optimum](#optimum-for-faster-latency)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "This section picks a leading model for classifying sentiment of chunks of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More text classification models: https://huggingface.co/models?pipeline_tag=text-classification&sort=trending\n",
    "model_name = \"SamLowe/roberta-base-go_emotions\" \n",
    "classifier_pipe = pipeline(\"text-classification\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I am feeling inspired today.\",\n",
    "    \"This talk is informative, but a bit high-level, where I can find more details?\",\n",
    "    \"I wonder about all the hype around Generative AI, is smoke and mirrors?\",\n",
    "    \"Building production machine learning systems is challenging.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_pipe(sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloom-560m\" # https://huggingface.co/bigscience/bloom-560m\n",
    "generator = pipeline(\"text-generation\", model=model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The Generative AI World Summit is a\"\n",
    "response = generator(prompt, do_sample=False, max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(f\"\"\"\n",
    "**Prompt**: {prompt}\n",
    "\n",
    "**{model_name}'s continuation**: {response[0]['generated_text']}...\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimum for Faster Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.pipelines import pipeline\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "prompt = \"I am attending the Generative AI Summit and I am a practicing [MASK].\"\n",
    "\n",
    "unmasked_optimum_pipeline = pipeline(task=\"fill-mask\", model=model_name, accelerator=\"bettertransformer\")\n",
    "response = unmasked_optimum_pipeline(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "col_mapping = {\"score\": \"Score\", \"token_str\": \"Token mask fill\", \"token\": \"Token ID\", \"sequence\": \"Full generated text\"}\n",
    "pd.DataFrame(response).rename(columns=col_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
