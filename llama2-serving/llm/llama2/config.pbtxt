name: "llama2"
backend: "python"
max_batch_size: 8
input [
  {
    name: "prompt"
    data_type: TYPE_STRING  
    dims: [-1]
  }
]
output [
  {
    name: "generated_text"
    data_type: TYPE_STRING  
    dims: [-1]
  }
]

instance_group [
  {
    kind: KIND_GPU
  }
]

dynamic_batching { 
  preferred_batch_size: [2, 4, 8] 
  max_queue_delay_microseconds: 3000000
}

